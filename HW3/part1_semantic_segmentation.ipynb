{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3710jvsc74a57bd09b6c46a2cfeb7771fba905868638ca0edd2605d074f7d01797e7e8649bfdb5ca","display_name":"Python 3.7.10 64-bit ('base37': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"part1_semantic_segmentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"LVIxz9O1N7g6"},"source":["# Homework 3.1: Dense Prediction\n","---\n","In this part, you will study a problem of segmentation. The goal of this assignment is to study, implement, and compare different components of dense prediction models, including **data augmentation**, **backbones**, **classifiers** and **losses**.\n","\n","This assignment will require training multiple neural networks, therefore it is advised to use a **GPU** accelerator."]},{"cell_type":"code","metadata":{"id":"_7EtbrFUN7hD"},"source":["# Uncomment and run if in Colab\n","# !mkdir datasets\n","# !gdown --id 139GsP9CqFCW1LA1Mf3e1gZpWz2uXmfHf -O datasets/tiny-floodnet-challenge.tar.gz\n","# !tar -xzf datasets/tiny-floodnet-challenge.tar.gz -C datasets\n","# !rm datasets/tiny-floodnet-challenge.tar.gz\n","# !gdown --id 1Td3RKkTsBEn1lBULddEmXKHxKhXqz_LC\n","# !tar -xzf part1_semantic_segmentation.tar.gz\n","# !rm part1_semantic_segmentation.tar.gz\n","\n","!pip install pytorch_lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from torch.nn import Conv2d, ReLU, BatchNorm2d, Dropout, ConvTranspose2d, MaxPool2d\n","\n","class DownBlock(torch.nn.Module):\n","    def __init__(self, in_ch, mid_ch = None , out_ch = None):\n","        super(DownBlock, self).__init__()\n","\n","        if out_ch == None: out_ch = 2*in_ch\n","        if mid_ch == None: mid_ch = out_ch\n","\n","        self.conv1 = Conv2d(in_channels = in_ch , out_channels = mid_ch, kernel_size = 3, stride = 1, padding = 1)\n","        self.batn1 = BatchNorm2d(mid_ch) \n","        self.act1  = ReLU()\n","        \n","        self.conv2 = Conv2d(in_channels = mid_ch , out_channels = out_ch , kernel_size = 3, stride = 1, padding = 1)\n","        self.batn2 = BatchNorm2d(out_ch)\n","        self.act2  = ReLU()\n","        \n","        self.pool  = MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.batn1(x) \n","        x = self.act1(x)\n","\n","        x = self.conv2(x)\n","        x = self.batn2(x)\n","        x = self.act2(x)\n","\n","        skip = x\n","\n","        x = self.pool(x)\n","\n","        return x, skip\n","\n","class BaseBlock(torch.nn.Module):\n","    def __init__(self, in_ch):\n","        super(BaseBlock, self).__init__()\n","        \n","        self.block = torch.nn.Sequential(\n","            Conv2d(in_channels=in_ch, out_channels = in_ch, kernel_size=3, stride=1, padding=1),\n","            Conv2d(in_channels=in_ch, out_channels = in_ch, kernel_size=3, stride=1, padding=1),\n","            ConvTranspose2d(in_channels = in_ch, out_channels= in_ch, kernel_size = 3, padding = 1, output_padding=1 , stride = 2)\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class UpBlock(torch.nn.Module):\n","    def __init__(self, in_ch , mid_ch = None, out_ch = None, isUpconv = True):\n","        super(UpBlock, self).__init__()\n","        \n","        if out_ch == None: out_ch = in_ch*2 // 4\n","        if mid_ch == None: mid_ch = out_ch\n","        #self.skip = skip\n","        self.isUpconv = isUpconv\n","\n","        self.drop1 = Dropout(p = 0.5)\n","        \n","        self.conv1 = Conv2d(in_channels = in_ch*2, out_channels = mid_ch, kernel_size = 3, stride= 1, padding=1)\n","        self.batn1 = BatchNorm2d(mid_ch)\n","        self.act1  = ReLU()\n","        \n","        self.conv2 = Conv2d(in_channels = mid_ch, out_channels = out_ch, kernel_size = 3, stride= 1, padding=1)\n","        self.batn2 = BatchNorm2d(out_ch)\n","        self.act2 = ReLU()\n","        \n","        if isUpconv:\n","            self.ups1  = ConvTranspose2d(in_channels = out_ch, out_channels= out_ch, kernel_size =3, padding =1, output_padding = 1, stride = 2)\n","        \n","\n","    def forward(self, x, skip):\n","\n","        x = torch.cat((skip, x), dim = 1)\n","        \n","        x = self.drop1(x)\n","        \n","        x = self.conv1(x)\n","        x = self.batn1(x)\n","        x = self.act1(x)\n","\n","        x = self.conv2(x)\n","        x = self.batn2(x)\n","        x = self.act2(x)\n","\n","        if self.isUpconv:\n","            x = self.ups1(x)\n","        \n","        return x \n","\n","class Unet(torch.nn.Module):\n","    def __init__(self):\n","        super(Unet, self).__init__()\n","\n","        self.down1 = DownBlock(1,56,112)\n","        self.down2 = DownBlock(112,224)\n","        self.down3 = DownBlock(224,448)\n","        self.base  = BaseBlock(448)\n","        self.up1   = UpBlock(448)\n","        self.up2   = UpBlock(224)\n","        self.up3   = UpBlock(112, 112, 56, False)\n","\n","    def forward(self, x):\n","        \n","        x, skip1 = self.down1(x)\n","        #print('down1: ', x.shape) \n","        x, skip2 = self.down2(x)\n","        #print('down2: ', x.shape)\n","        x,skip3 = self.down3(x)\n","        #print('down3: ', x.shape)\n","        x = self.base(x)\n","        #print('base: ', x.shape)\n","        x = self.up1(x,skip3)\n","        #print('up1: ', x.shape)\n","        x = self.up2(x,skip2)\n","        #print('up2: ', x.shape)\n","        x = self.up3(x,skip1)\n","\n","        \n","        return x\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from torch.nn import Conv2d, ReLU, BatchNorm2d, Dropout, ConvTranspose2d, MaxPool2d\n","\n","class DownBlock(torch.nn.Module):\n","    def __init__(self, in_ch, mid_ch = None , out_ch = None):\n","        super(DownBlock, self).__init__()\n","\n","        if out_ch == None: out_ch = 2*in_ch\n","        if mid_ch == None: mid_ch = out_ch\n","\n","        self.block = nn.Sequential(\n","            Conv2d(in_channels = in_ch , out_channels = mid_ch, kernel_size = 3, stride = 1, padding = 1),\n","            #BatchNorm2d(mid_ch),\n","            ReLU(), \n","\n","            Conv2d(in_channels = mid_ch , out_channels = out_ch , kernel_size = 3, stride = 1, padding = 1),\n","            #BatchNorm2d(out_ch),\n","            ReLU()\n","        )\n","\n","        self.pool  = MaxPool2d(kernel_size = 2, stride = 2, padding = 0)\n","    \n","    def forward(self, x):\n","\n","        x = self.block(x)\n","        \n","        skip = x\n","\n","        x = self.pool(x)\n","\n","        return x, skip\n","\n","class BaseBlock(torch.nn.Module):\n","    def __init__(self, in_ch):\n","        super(BaseBlock, self).__init__()\n","        \n","        self.block = torch.nn.Sequential(\n","            Conv2d(in_channels=in_ch, out_channels = in_ch, kernel_size=3, stride=1, padding=1),\n","            #BatchNorm2d(in_ch),\n","            ReLU(),\n","            Conv2d(in_channels=in_ch, out_channels = in_ch, kernel_size=3, stride=1, padding=1),\n","            #BatchNorm2d(in_ch),\n","            ReLU(),\n","            ConvTranspose2d(in_channels = in_ch, out_channels= in_ch, kernel_size = 3, padding = 1, output_padding=1 , stride = 2)\n","        )\n","\n","    def forward(self, x):\n","        return self.block(x)\n","\n","\n","class UpBlock(torch.nn.Module):\n","    def __init__(self, in_ch , mid_ch = None, out_ch = None, isUpconv = True):\n","        super(UpBlock, self).__init__()\n","        \n","        if out_ch == None: out_ch = in_ch*2 // 4\n","        if mid_ch == None: mid_ch = out_ch\n","        #self.skip = skip\n","        self.isUpconv = isUpconv\n","\n","        self.block = nn.Sequential(\n","            Dropout(p = 0.5),\n","            Conv2d(in_channels = in_ch*2, out_channels = mid_ch, kernel_size = 3, stride= 1, padding=1),\n","            #BatchNorm2d(mid_ch),\n","            ReLU(),\n","\n","            Conv2d(in_channels = mid_ch, out_channels = out_ch, kernel_size = 3, stride= 1, padding=1),\n","            #BatchNorm2d(out_ch),\n","            ReLU()\n","        )\n","\n","        if isUpconv:\n","            self.ups1  = ConvTranspose2d(in_channels = out_ch, out_channels= out_ch, kernel_size =3, padding =1, output_padding = 1, stride = 2)\n","        \n","\n","    def forward(self, x, skip):\n","\n","        x = torch.cat((skip, x), dim = 1)\n","        \n","        x = self.block(x)\n","\n","        if self.isUpconv:\n","            x = self.ups1(x)\n","        \n","        return x \n","\n","class Unet(torch.nn.Module):\n","    def __init__(self):\n","        super(Unet, self).__init__()\n","\n","        self.down1 = DownBlock(1,56,112)\n","        self.down2 = DownBlock(112,224)\n","        self.down3 = DownBlock(224,448)\n","        self.base  = BaseBlock(448)\n","        self.up1   = UpBlock(448)\n","        self.up2   = UpBlock(224)\n","        self.up3   = UpBlock(112, 112, 56, False)\n","\n","    def forward(self, x):\n","        \n","        x, skip1 = self.down1(x)\n","        #print('down1: ', x.shape) \n","        x, skip2 = self.down2(x)\n","        #print('down2: ', x.shape)\n","        x,skip3 = self.down3(x)\n","        #print('down3: ', x.shape)\n","        x = self.base(x)\n","        #print('base: ', x.shape)\n","        x = self.up1(x,skip3)\n","        #print('up1: ', x.shape)\n","        x = self.up2(x,skip2)\n","        #print('up2: ', x.shape)\n","        x = self.up3(x,skip1)\n","\n","        \n","        return x\n","\n"]},{"source":["import torch \n","from torch import nn\n","class UnetDownBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, pooling=True):\n","        super().__init__()\n","        self.convs = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","        self.maxpool = nn.MaxPool2d(kernel_size=2)\n","        \n","    def forward(self, x):\n","        out_before_pooling = self.convs(x)\n","        out = self.maxpool(out_before_pooling)\n","\n","        return out, out_before_pooling\n","    \n","class UnetUpBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.upsample = nn.Upsample(scale_factor=2)\n","        self.convs = nn.Sequential(\n","            nn.Conv2d(in_channels * 2, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","        \n","    def forward(self, x, x_bridge):\n","        x_up = self.upsample(x)\n","        x_concat = torch.cat([x_up, x_bridge], dim=1)\n","        out = self.convs(x_concat)\n","        \n","        return out\n","\n","class Unet(nn.Module):\n","    def __init__(self, n_base_channels=64):\n","        super().__init__()\n","        self.down_blocks = nn.ModuleList([\n","            UnetDownBlock(3, n_base_channels),\n","            UnetDownBlock(n_base_channels, n_base_channels * 2),\n","            UnetDownBlock(n_base_channels * 2, n_base_channels * 4),\n","            UnetDownBlock(n_base_channels * 4, n_base_channels * 4),\n","            UnetDownBlock(n_base_channels * 4, n_base_channels * 4)\n","        ])\n","        self.up_blocks = nn.ModuleList([\n","            UnetUpBlock(n_base_channels * 4, n_base_channels * 4),\n","            UnetUpBlock(n_base_channels * 4, n_base_channels * 2),\n","            UnetUpBlock(n_base_channels * 2, n_base_channels),\n","            UnetUpBlock(n_base_channels, n_base_channels),\n","        ])\n","        self.final_block = nn.Sequential(\n","            nn.Conv2d(n_base_channels, 1, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","            \n","        \n","    def forward(self, x):\n","        out = x\n","        outputs_before_pooling = []\n","        for i, block in enumerate(self.down_blocks):\n","            out, before_pooling = block(out)\n","            outputs_before_pooling.append(before_pooling)\n","        out = before_pooling\n","        \n","        for i, block in enumerate(self.up_blocks):\n","            out = block(out, outputs_before_pooling[-i - 2])\n","        out = self.final_block(out)\n","        \n","        return out"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Unet(\n  (down_blocks): ModuleList(\n    (0): UnetDownBlock(\n      (convs): Sequential(\n        (0): Conv2d(3, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (1): UnetDownBlock(\n      (convs): Sequential(\n        (0): Conv2d(56, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (2): UnetDownBlock(\n      (convs): Sequential(\n        (0): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (3): UnetDownBlock(\n      (convs): Sequential(\n        (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n    (4): UnetDownBlock(\n      (convs): Sequential(\n        (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    )\n  )\n  (up_blocks): ModuleList(\n    (0): UnetUpBlock(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): Sequential(\n        (0): Conv2d(448, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): UnetUpBlock(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): Sequential(\n        (0): Conv2d(448, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (2): UnetUpBlock(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): Sequential(\n        (0): Conv2d(224, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (3): UnetUpBlock(\n      (upsample): Upsample(scale_factor=2.0, mode=nearest)\n      (convs): Sequential(\n        (0): Conv2d(112, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): ReLU()\n        (2): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (3): ReLU()\n        (4): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (final_block): Sequential(\n    (0): Conv2d(56, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Sigmoid()\n  )\n)\n"]}],"source":["print(unet)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["unet = Unet()\n","tens = torch.rand(1,1,1000,1000)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["unet = unet.to(device)\n","tens = tens.to(device)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 5.81 GiB total capacity; 4.24 GiB already allocated; 64.44 MiB free; 4.30 GiB reserved in total by PyTorch)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-eabbab56bd05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ca882ec5dc8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m#print('down2: ', x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;31m#print('down3: ', x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ca882ec5dc8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/base37/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1204\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 5.81 GiB total capacity; 4.24 GiB already allocated; 64.44 MiB free; 4.30 GiB reserved in total by PyTorch)"]}],"source":["res = unet(tens)\n","res.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["res.shape"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["test = torch.nn.Sequential(\n","    DownBlock(1,56,112),\n","    DownBlock(112,224),\n","    DownBlock(224,448),\n","    BaseBlock(448),\n","    \n",")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["skip = torch.rand(10,448,64,64)\n","test = UpBlock(in_ch = 448)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 448, 64, 64])"]},"metadata":{},"execution_count":20}],"source":["tens = torch.rand(10, 448, 64,64)\n","tens.shape"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 224, 128, 128])"]},"metadata":{},"execution_count":22}],"source":["res = test.forward(tens, tens)\n","res.shape"]},{"cell_type":"markdown","metadata":{"id":"tI6a6z4eN7hE"},"source":["## Dataset\n","\n","We will use a simplified version of a [FloodNet Challenge](http://www.classic.grss-ieee.org/earthvision2021/challenge.html).\n","\n","Compared to the original challenge, our version doesn't have difficult (and rare) \"flooded\" labels, and the images are downsampled\n","\n","<img src=\"https://i.imgur.com/RZuVuVp.png\" />"]},{"cell_type":"markdown","metadata":{"id":"vf0zwWxrN7hF"},"source":["## Assignments and grading\n","\n","\n","- **Part 1. Code**: fill in the empty gaps (marked with `#TODO`) in the code of the assignment (34 points):\n","    - `dataset.py` -- 4 points\n","    - `model.py` -- 22 points\n","    - `loss.py` -- 6 points\n","    - `train.py` -- 2 points\n","- **Part 2. Train and benchmark** the performance of the required models (6 points):\n","    - All 6 checkpoints are provided -- 3 points\n","    - Checkpoints have > 0.5 accuracy -- 3 points\n","- **Part 3. Report** your findings (10 points)\n","    - Each task -- 2.5 points\n","\n","- **Total score**: 50 points.\n","\n","For detailed grading of each coding assignment, please refer to the comments inside the files. Please use the materials provided during a seminar and during a lecture to do a coding part, as this will help you to further familiarize yourself with PyTorch. Copy-pasting the code from Google Search will get penalized.\n","\n","In part 2, you should upload all your pre-trained checkpoints to your personal Google Drive, grant public access and provide a file ID, following the intructions in the notebook.\n","\n","Note that for each task in part 3 to count towards your final grade, you should complete the corresponding tasks in part 2.\n","\n","For example, if you are asked to compare Model X and Model Y, you should provide the checkpoints for these models in your submission, and their accuracies should be above minimal threshold."]},{"cell_type":"markdown","metadata":{"id":"tuS-7JPoSD6Q"},"source":["## Part 1. Code\n"]},{"cell_type":"markdown","metadata":{"id":"DziabO6AN7hF"},"source":["### `dataset.py`\n","**TODO: implement and apply data augmentations**\n","\n","You'll need to study a popular augmentations library: [Albumentations](https://albumentations.ai/), and implement the requested augs. Remember that geometric augmentations need to be applied to both images and masks at the same time, and Albumentations has [native support](https://albumentations.ai/docs/getting_started/mask_augmentation/) for that."]},{"cell_type":"markdown","metadata":{"id":"y-Qcudg1N7hG"},"source":["### `model.py`\n","**TODO: Implement the required models.**\n","\n","Typically, all segmentation networks consist of an encoder and decoder. Below is a scheme for a popular DeepLab v3 architecture:\n","\n","<img src=\"https://i.imgur.com/cdlkxvp.png\" />\n","\n","The encoder consists of a convolutional backbone, typically with extensive use of convs with dilations (atrous convs) and a head, which helps to further boost the receptive field. As you can see, the general idea for the encoders is to have as big of a receptive field, as possible.\n","\n","The decoder either does upsampling with convolutions (similarly to the scheme above, or to UNets), or even by simply interpolating the outputs of the encoder.\n","\n","In this assignment, you will need to implement **UNet** and **DeepLab** models. Example UNet looks like this:\n","\n","<img src=\"https://i.imgur.com/RJyO1rV.png\" />\n","\n","For **DeepLab** model we will have three variants for backbones: **ResNet18**, **VGG11 (with BatchNorm)**, and **MobileNet v3 (small).** Use `torchvision.models` to obtain pre-trained versions of these backbones and simply extract their convolutional parts. To familiarize yourself with **MobileNet v3** model, follow this [link](https://paperswithcode.com/paper/searching-for-mobilenetv3).\n","\n","We will also use **Atrous Spatial Pyramid Pooling (ASPP)** head. Its scheme can be seen in the DeepLab v3 architecture above. ASPP is one of the blocks which greatly increases the spatial size of the model, and hence boosts the model's performance. For more details, you can refer to this [link](https://paperswithcode.com/method/aspp)."]},{"cell_type":"markdown","metadata":{"id":"8VDn5sE3N7hH"},"source":["### `loss.py`\n","**TODO: implement test losses.**\n","\n","For validation, we will use three metrics. \n","- Mean intersection over union: **mIoU**,\n","- Mean class recall: **mRecall**,\n","- **Accuracy**.\n","\n","To calculate **IoU**, use this formula for binary segmentation masks for each class, and then average w.r.t. all classes:\n","\n","$$ \\text{IoU} = \\frac{ \\text{area of intersection} }{ \\text{area of union} } = \\frac{ \\| \\hat{m} \\cap m  \\| }{ \\| \\hat{m} \\cup m \\| }, \\quad \\text{$\\hat{m}$ — predicted binary mask},\\ \\text{$m$ — target binary mask}.$$\n","\n","For **mRecall** you can use the following formula:\n","\n","$$\n","    \\text{mRecall} = \\frac{ \\| \\hat{m} \\cap m \\| }{ \\| m \\| }\n","$$\n","\n","And **accuracy** is a fraction of correctly identified pixels in the image.\n","\n","Generally, we want our models to optimize accuracy since this implies that it makes little mistakes. However, most of the segmentation problems have imbalanced classes, and therefore the models tend to underfit the rare classes. Therefore, we also need to measure the mean performance of the model across all classes (mean IoU or mean class accuracy). In reality, these metrics (not the accuracy) are the go-to benchmarks for segmentation models."]},{"cell_type":"markdown","metadata":{"id":"aRiIQ1_5N7hH"},"source":["### `train.py`\n","**TODO: define optimizer and learning rate scheduler.**\n","\n","You need to experiment with different optimizers and schedulers and pick one of each which works the best. Since the grading will be partially based on the validation performance of your models, we strongly advise doing some preliminary experiments and pick the configuration with the best results."]},{"cell_type":"markdown","metadata":{"id":"Hi3TDmYyN7hI"},"source":["## Part 2. Train and benchmark\n","\n","In this part of the assignment, you need to train the following models and measure their training time:\n","- **UNet** (with and without data augmentation),\n","- **DeepLab** with **ResNet18** backbone (with **ASPP** = True and False),\n","- **DeepLab** with the remaining backbones you implemented and **ASPP** = True).\n","\n","To get the full mark for this assignment, all the required models should be trained (and their checkpoints provided), and have at least 0.5 accuracies.\n","\n","After the models are trained, evaluate their inference time on both GPU and CPU.\n","\n","Example training and evaluation code are below."]},{"cell_type":"code","metadata":{"id":"tweq-S1e8jnU"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFKHCO_DN7hJ"},"source":["import pytorch_lightning as pl\n","from part1_semantic_segmentation.train import SegModel\n","import time\n","import torch\n","\n","\n","\n","def define_model(model_name: str, \n","                 backbone: str, \n","                 aspp: bool, \n","                 augment_data: bool, \n","                 optimizer: str, \n","                 scheduler: str, \n","                 lr: float, \n","                 checkpoint_name: str = '', \n","                 batch_size: int = 16):\n","    assignment_dir = 'part1_semantic_segmentation'\n","    experiment_name = f'{model_name}_{backbone}_augment={augment_data}_aspp={aspp}'\n","    model_name = model_name.lower()\n","    backbone = backbone.lower() if backbone is not None else backbone\n","    \n","    model = SegModel(\n","        model_name, \n","        backbone, \n","        aspp, \n","        augment_data,\n","        optimizer,\n","        scheduler,\n","        lr,\n","        batch_size, \n","        data_path='datasets/tiny-floodnet-challenge', \n","        image_size=256)\n","\n","    if checkpoint_name:\n","        model.load_state_dict(torch.load(f'{assignment_dir}/logs/{experiment_name}/{checkpoint_name}')['state_dict'])\n","    \n","    return model, experiment_name\n","\n","def train(model, experiment_name, use_gpu):\n","    assignment_dir = 'part1_semantic_segmentation'\n","\n","    logger = pl.loggers.TensorBoardLogger(save_dir=f'{assignment_dir}/logs', name=experiment_name)\n","\n","    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","        monitor='mean_iou',\n","        dirpath=f'{assignment_dir}/logs/{experiment_name}',\n","        filename='{epoch:02d}-{mean_iou:.3f}',\n","        mode='max')\n","    \n","    trainer = pl.Trainer(\n","        max_epochs=100, \n","        gpus=1 if use_gpu else None, \n","        benchmark=True, \n","        check_val_every_n_epoch=5, \n","        logger=logger, \n","        callbacks=[checkpoint_callback])\n","\n","    time_start = time.time()\n","    \n","    trainer.fit(model)\n","    \n","    torch.cuda.synchronize()\n","    time_end = time.time()\n","    \n","    training_time = (time_end - time_start) / 60\n","    \n","    return training_time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xj3nksa7N7hK"},"source":["model, experiment_name = define_model(\n","    model_name='UNet',\n","    backbone=None,\n","    aspp=None,\n","    augment_data=False,\n","    optimizer='', # use these options to experiment\n","    scheduler='', # with optimizers and schedulers\n","    lr=1.) # experiment to find the best LR\n","training_time = train(model, experiment_name, use_gpu=False)\n","\n","print(f'Training time: {training_time:.3f} minutes')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhZ7Lxl1N7hK"},"source":["After training, the loss curves and validation images with their segmentation masks can be viewed using the TensorBoard extension:"]},{"cell_type":"code","metadata":{"id":"hk5ZiCKZN7hL"},"source":["%load_ext tensorboard\n","%tensorboard --logdir part1_semantic_segmentation/logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u_fRLdGlN7hL"},"source":["Inference time can be measured via the following function:"]},{"cell_type":"code","metadata":{"id":"oiSMdtO4N7hM"},"source":["def calc_inference_time(model, device, input_shape=(1000, 750), num_iters=100):\n","    timings = []\n","\n","    for i in range(num_iters):\n","        x = torch.randn(1, 3, *input_shape).to(device)\n","        time_start = time.time()\n","        \n","        model(x)\n","        \n","        torch.cuda.synchronize()\n","        time_end = time.time()\n","        \n","        timings.append(time_end - time_start)\n","\n","    return sum(timings) / len(timings) * 1e3\n","\n","\n","model = define_model(\n","    model_name='unet',\n","    backbone=None,\n","    aspp=None,\n","    augment_data=False,\n","    checkpoint_name=<TODO>)\n","\n","inference_time = calc_inference_time(model.eval().cpu(), 'cpu')\n","# inference_time = calc_inference_time(model.eval().cuda(), 'cuda')\n","\n","print(f'Inferece time (per frame): {inference_time:.3f} ms')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lh2ilbnoJzJ0"},"source":["Your trained weights are available in the `part1_semantic_segmentation/logs` folder. Inside, your experiment directory has a log file with the following mask: `{epoch:02d}-{mean_iou:.3f}.ckpt`. Make sure that you models satisfy the accuracy requirements, upload them to your personal Google Drive. Provide file ids and checksums below. Use `!md5sum <PATH>` to compute the checksums.\n","\n","To make sure that provided ids are correct, try running `!gdown --id <ID>` command from this notebook."]},{"cell_type":"code","metadata":{"id":"fVXnmnSDJ1Kf"},"source":["checkpoint_ids = {\n","    'UNet_None_augment=False_aspp=None': (<ID>, <CHECKSUM>), # TODO\n","    'UNet_None_augment=True_aspp=None': None, # TODO\n","    'DeepLab_ResNet18_augment=True_aspp=False': None, # TODO\n","    'DeepLab_ResNet18_augment=True_aspp=True': None, # TODO\n","    'DeepLab_VGG11_bn_augment=True_aspp=True': None, # TODO\n","    'DeepLab_MobileNet_v3_small_augment=True_aspp=True': None, # TODO\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_2oX5M2ZN7hM"},"source":["## Part 3. Report\n","\n","You should have obtained 6 different models, which we will use for the comparison and evaluation. When asked to visualize specific loss curves, simply configure these plots in TensorBoard, screenshot, store them in the `report` folder, and load into Jupyter markdown:\n","\n","`<img src=\"./part1_semantic_segmentation/report/<screenshot_filename>\"/>`\n","\n","If you have problems loading these images, try uploading them [here](https://imgur.com) and using a link as `src`. Do not forget to include the raw files in the `report` folder anyways.\n","\n","You should make sure that your plots satisfy the following requirements:\n","- Each plot has a title,\n","- If there are multiple curves on one plot (or dots on the scatter plot), the plot legend should also be present,\n","- If the plot is not obtained using TensorBoard (Task 3), the axis should have names and ticks."]},{"cell_type":"markdown","metadata":{"id":"_-vODgPbN7hN"},"source":["#### Task 1.\n","Visualize training loss and validation loss curves for UNet trained with and without data augmentation. What are the differences in the behavior of these curves between these experiments, and what are the reasons?"]},{"cell_type":"markdown","metadata":{"id":"Wl6Q2UtDN7hN"},"source":["TODO"]},{"cell_type":"markdown","metadata":{"id":"hL14-DsON7hN"},"source":["#### Task 2.\n","Visualize training and validation loss curves for ResNet18 trained with and without ASPP. Which model performs better?"]},{"cell_type":"markdown","metadata":{"id":"GdGq0hUbN7hO"},"source":["TODO"]},{"cell_type":"markdown","metadata":{"id":"SCIs_q4WN7hO"},"source":["#### Task 3.\n","Compare **UNet** with augmentations and **DeepLab** with all backbones (only experiments with **ASPP**). To do that, put these models on three scatter plots. For the first plot, the x-axis is **training time** (in minutes), for the second plot, the x-axis is **inference time** (in milliseconds), and for the third plot, the x-axis is **model size** (in megabytes). The size of each model is printed by PyTorch Lightning. For all plots, the y-axis is the best **mIoU**. To clarify, each of the **4** requested models should be a single dot on each of these plots.\n","\n","Which models are the most efficient with respect to each metric on the x-axes? For each of the evaluated models, rate its performance using their validation metrics, training and inference time, and model size. Also for each model explain what are its advantages, and how its performance could be improved?"]},{"cell_type":"markdown","metadata":{"id":"TiNRa25cN7hO"},"source":["TODO"]},{"cell_type":"markdown","metadata":{"id":"C6FyZ7wqN7hO"},"source":["#### Task 4.\n","\n","Pick the best model according to **mIoU** and look at the visualized predictions on the validation set in the TensorBoard. For each segmentation class, find the good examples (if they are available), and the failure cases. Provide the zoomed-in examples and their analysis below. Please do not attach full validation images, only the areas of interest which you should crop manually."]},{"cell_type":"markdown","metadata":{"id":"8YIv1z-aN7hP"},"source":["TODO"]}]}