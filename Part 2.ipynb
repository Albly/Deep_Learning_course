{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3710jvsc74a57bd09b6c46a2cfeb7771fba905868638ca0edd2605d074f7d01797e7e8649bfdb5ca","display_name":"Python 3.7.10 64-bit ('base37': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"Part 2.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["\n     active environment : base37\n    active env location : /home/alex/anaconda3/envs/base37\n            shell level : 2\n       user config file : /home/alex/.condarc\n populated config files : /home/alex/.condarc\n          conda version : 4.9.2\n    conda-build version : 3.20.5\n         python version : 3.8.5.final.0\n       virtual packages : __cuda=11.2=0\n                          __glibc=2.31=0\n                          __unix=0=0\n                          __archspec=1=x86_64\n       base environment : /home/alex/anaconda3  (writable)\n           channel URLs : https://conda.anaconda.org/conda-forge/linux-64\n                          https://conda.anaconda.org/conda-forge/noarch\n                          https://repo.anaconda.com/pkgs/main/linux-64\n                          https://repo.anaconda.com/pkgs/main/noarch\n                          https://repo.anaconda.com/pkgs/r/linux-64\n                          https://repo.anaconda.com/pkgs/r/noarch\n          package cache : /home/alex/anaconda3/pkgs\n                          /home/alex/.conda/pkgs\n       envs directories : /home/alex/anaconda3/envs\n                          /home/alex/.conda/envs\n               platform : linux-64\n             user-agent : conda/4.9.2 requests/2.24.0 CPython/3.8.5 Linux/5.8.0-49-generic ubuntu/20.04.2 glibc/2.31\n                UID:GID : 1000:1000\n             netrc file : /home/alex/.netrc\n           offline mode : False\n\n"]}],"source":["!conda info"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"gFe273vKFgF-"},"source":["# Homework 2\n","## Part 2 (60 points total)\n","\n","In this part, you will build a convolutional neural network (aka ConvNet or CNN) to solve yet another image classification problem: the Tiny ImageNet dataset (200 classes, 100K training images, 10K validation images). Try to achieve as high accuracy as possible.\n","\n","This exercise is close to what people do in real life. No toy architectures this time. **Unlike in part 1**, you are now free to use the full power of PyTorch and its submodules."]},{"cell_type":"markdown","metadata":{"id":"-S-WyYK8FgGF"},"source":["## Grading\n","\n","* 11 points for the report.\n","* 5 points for using an **interactive** (please don't reinvent the wheel with `plt.plot`) tool for viewing progress, for example TensorBoard.\n","* 9 points for a network that gets above 25% accuracy on the private **test** set.\n","* Up to 35 points for accuracy up to 50%, issued linearly (i.e. 0 points for 25%, 7 points for 30%, 21 points for 40%, 35 points for $\\geq$50%.\n","\n","## Grading Explained\n","\n","* *Private test set*: it's a part of the dataset like the validation set, but for which the ground truth labels are known only to us (you won't be able to evaluate your model on it). When grading, we will compute test accuracy by running your code that computes val accuracy, but having replaced the images in `'val/'` with the test set.\n","* *Submitting a neural net*:\n","  * **<font color=\"red\">Wrong checkpoint submission = zero points for accuracy. Be careful!</font>**\n","  * After you've trained your network, [save weights](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) to \"*checkpoint.pth*\" with `model.state_dict()` and `torch.save()`.\n","  * Set `DO_TRAIN = False`, click \"Restart and Run All\" and make sure that your validation accuracy is computed correctly.\n","  * Compute the MD5 checksum for \"*checkpoint.pth*\" (e.g. run `!md5sum checkpoint.pth`) and paste it into \"*part2_solution.py*\" (`get_checkpoint_metadata()`). You'll be penalized if this checksum doesn't match your submitted file.\n","  * Upload \"*checkpoint.pth*\" to Google Drive, copy the view-only link to it and paste it into \"*part2_solution.py*\" as well.\n","* *Report*: PDF, free form; rough list of points to touch upon:\n","  * Your history of tweaks and improvements. How you started, what you searched. (*I have analyzed these and those conference papers/sources/blog posts. I tried this and that to adapt them to my problem. ...*)\n","  * Which network architectures have you tried? Which of them didn't work, and can you guess why? What is the final one and why?\n","  * Same for the training method (batch size, optimization algorithm, number of iterations, ...): which and why?\n","  * Same for anti-overfitting (regularization) techniques. Which ones have you tried? What were their effects, and can you guess why?\n","  * **Most importantly**: deep learning insights you gained. Can you give several examples of how *exactly* experience from this exercise will affect you training your future neural nets? (tricks, heuristics, conclusions, observations)\n","  * **List all sources of code**.\n","* *Progress viewing tool*: support the report with screenshots of accuracy and loss plots (training and validation) over time.\n","\n","## Restrictions\n","\n","* No pretrained networks.\n","* Don't enlarge images (e.g. don't resize them to $224 \\times 224$ or $256 \\times 256$).\n","\n","## Tips\n","\n","* **One change at a time**: don't test several new things at once (unless you are super confident that they will work). Train a model, introduce one change, train again.\n","* Google a lot: try to reinvent as few wheels as possible (unlike in part 1 of this assignment). Harvest inspiration from PyTorch recipes, from GitHub, from blogs...\n","* Use GPU.\n","* Regularization is very important: L2, batch normalization, dropout, data augmentation...\n","* Pay much attention to accuracy and loss graphs (e.g. in TensorBoard). Track failures early, stop bad experiments early.\n","* 2-3 hours of training (in Colab) should be enough for most models, maybe 4-6 hours if you're experimenting.\n","* Save checkpoints every so often in case things go wrong (optimization diverges, Colab disconnects...).\n","* Don't use too large batches, they can be slow and memory-hungry. This is true for inference too.\n","* Also don't forget to use `torch.no_grad()` and `.eval()` during inference."]},{"cell_type":"code","metadata":{"id":"m2qxvLGdFgGH"},"source":["# Determine the locations of auxiliary libraries and datasets.\n","# `AUX_DATA_ROOT` is where 'notmnist.py', 'animation.py' and 'tiny-imagenet-2020.zip' are.\n","\n","# Detect if we are in Google Colaboratory\n","try:\n","    import google.colab\n","    IN_COLAB = True\n","except ImportError:\n","    IN_COLAB = False\n","\n","from pathlib import Path\n","if IN_COLAB:\n","    google.colab.drive.mount(\"/content/drive\")\n","    \n","    # Change this if you created the shortcut in a different location\n","    AUX_DATA_ROOT = Path(\"/content/drive/My Drive/Deep Learning 2021 -- Home Assignment 2\")\n","    \n","    assert AUX_DATA_ROOT.is_dir(), \"Have you forgot to 'Add a shortcut to Drive'?\"\n","    \n","    import sys\n","    sys.path.append(str(AUX_DATA_ROOT))\n","else:\n","    AUX_DATA_ROOT = Path(\".\")"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcFGHHXVsM-J"},"source":["# Imports\n","\n","# Your solution\n","%load_ext autoreload\n","%autoreload 1\n","\n","%aimport part2_solution"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5eKWQv9wi3P"},"source":["# If `True`, will train the model from scratch and validate it.\n","# If `False`, instead of training will load weights from './checkpoint.pth'.\n","# When grading, we will test both cases.\n","DO_TRAIN = True"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"njk50aDoFgGT"},"source":["# Put training and validation images in `./tiny-imagenet-200/train` and `./tiny-imagenet-200/val`:\n","if not Path(\"tiny-imagenet-200/train/class_000/00000.jpg\").is_file():\n","    import zipfile\n","    with zipfile.ZipFile(AUX_DATA_ROOT / 'tiny-imagenet-2020.zip', 'r') as archive:\n","        archive.extractall()"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFqnb1-EFgGj"},"source":["# Initialize dataloaders\n","train_dataloader = part2_solution.get_dataloader(\"./tiny-imagenet-200/\", 'train')\n","val_dataloader   = part2_solution.get_dataloader(\"./tiny-imagenet-200/\", 'val')\n","\n","# Initialize the raw model\n","model = part2_solution.get_model()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9n7DyGcFgGq"},"source":["if DO_TRAIN:\n","    # Train from scratch\n","    optimizer = part2_solution.get_optimizer(model)\n","    part2_solution.train_on_tinyimagenet(train_dataloader, val_dataloader, model, optimizer)\n","else:\n","    # Load from disk\n","    part2_solution.load_weights(model, \"./checkpoint.pth\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/100 [00:00<?, ?it/s]Starting Training using device :  cuda:0\n","  1%|          | 1/100 [03:49<6:19:02, 229.73s/it]Validation accuracy:19.68%\n","  2%|▏         | 2/100 [07:36<6:12:50, 228.27s/it]Validation accuracy:25.63%\n","  3%|▎         | 3/100 [11:25<6:09:03, 228.29s/it]Validation accuracy:24.92%\n","  4%|▍         | 4/100 [15:10<6:03:31, 227.21s/it]Validation accuracy:32.30%\n","  5%|▌         | 5/100 [18:54<5:57:44, 225.95s/it]Validation accuracy:35.44%\n","  6%|▌         | 6/100 [22:36<5:51:58, 224.67s/it]Validation accuracy:34.64%\n","  7%|▋         | 7/100 [26:17<5:46:04, 223.27s/it]Validation accuracy:38.36%\n","  8%|▊         | 8/100 [29:58<5:41:17, 222.58s/it]Validation accuracy:34.96%\n","  9%|▉         | 9/100 [33:36<5:35:29, 221.21s/it]Validation accuracy:39.34%\n"," 10%|█         | 10/100 [37:17<5:31:47, 221.19s/it]Validation accuracy:41.41%\n"," 11%|█         | 11/100 [40:57<5:27:24, 220.73s/it]Validation accuracy:42.43%\n"," 12%|█▏        | 12/100 [44:34<5:21:58, 219.52s/it]Validation accuracy:41.89%\n"," 13%|█▎        | 13/100 [48:10<5:17:09, 218.73s/it]Validation accuracy:42.81%\n"," 14%|█▍        | 14/100 [51:47<5:12:47, 218.23s/it]Validation accuracy:42.53%\n"," 15%|█▌        | 15/100 [55:24<5:08:34, 217.82s/it]Validation accuracy:41.75%\n"," 16%|█▌        | 16/100 [59:01<5:04:31, 217.52s/it]Validation accuracy:40.54%\n"," 17%|█▋        | 17/100 [1:02:38<5:00:36, 217.31s/it]Validation accuracy:43.92%\n"," 18%|█▊        | 18/100 [1:06:15<4:56:44, 217.13s/it]Validation accuracy:44.56%\n"," 19%|█▉        | 19/100 [1:09:51<4:52:53, 216.96s/it]Validation accuracy:43.40%\n"," 20%|██        | 20/100 [1:13:28<4:49:10, 216.88s/it]Validation accuracy:43.77%\n"," 21%|██        | 21/100 [1:17:05<4:45:26, 216.79s/it]Validation accuracy:49.26%\n"," 22%|██▏       | 22/100 [1:20:41<4:41:44, 216.73s/it]Validation accuracy:49.39%\n"," 23%|██▎       | 23/100 [1:24:18<4:38:05, 216.69s/it]Validation accuracy:49.49%\n"," 24%|██▍       | 24/100 [1:27:54<4:34:27, 216.68s/it]Validation accuracy:49.71%\n"," 24%|██▍       | 24/100 [1:29:56<4:44:48, 224.85s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-78b89c551efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpart2_solution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpart2_solution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_tinyimagenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Load from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/GitHub/Deep_Learning_course/part2_solution.py\u001b[0m in \u001b[0;36mtrain_on_tinyimagenet\u001b[0;34m(train_dataloader, val_dataloader, model, optimizer)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"CDJw8MokFxP9"},"source":["# Classify some validation samples\n","example_batch, example_batch_labels = next(iter(val_dataloader))\n","_, example_predicted_labels = part2_solution.predict(model, example_batch).max(1)\n","\n","print(\"Predicted class / Ground truth class\")\n","for predicted, gt in list(zip(example_predicted_labels, example_batch_labels))[:15]:\n","    print(\"{:03d} / {:03d}\".format(predicted, gt))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class / Ground truth class\n000 / 000\n128 / 000\n000 / 000\n062 / 000\n000 / 000\n079 / 000\n000 / 000\n000 / 000\n000 / 000\n174 / 000\n000 / 000\n000 / 000\n000 / 000\n000 / 000\n000 / 000\n"]}]},{"cell_type":"code","metadata":{"id":"U_Qddecy7-uS"},"source":["# Print validation accuracy\n","val_accuracy, _ = part2_solution.validate(val_dataloader, model)\n","val_accuracy *= 100\n","assert 1.5 <= val_accuracy <= 100.0\n","print(\"Validation accuracy: %.2f%%\" % val_accuracy)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation accuracy: 49.76%\n"]}]},{"cell_type":"code","metadata":{"id":"URNOeZ0MSZuq"},"source":["md5_checksum, google_drive_link = part2_solution.get_checkpoint_metadata()\n","print(f\"Claimed MD5 checksum: {md5_checksum}\")\n","print(\"Real MD5 checksum:\")\n","!md5sum checkpoint.pth"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Claimed MD5 checksum: 747822ca4436819145de8f9e410ca9ca\nReal MD5 checksum:\n6acd9a9ac8758dc0508b1775ba672516  checkpoint.pth\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}